{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5khYGCivYcmZ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from pathlib import Path\n",
        "import itertools\n",
        "from functools import reduce\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "\n",
        "from tabulate import tabulate\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "User parameters"
      ],
      "metadata": {
        "id": "JFgjqDxVYe26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bandpass_filename = 'SH_Round_Data.xlsx'\n",
        "test_ratio = 0.3  # Proportion of test data\n",
        "random_state = 42  # Specify to fix splitted training and test data. Otherwise, set to None.\n",
        "labels = {\n",
        "    0: 'NON',\n",
        "    1: 'TAR'\n",
        "}\n",
        "n_splits = 5\n",
        "max_input_dim = 100\n",
        "rounds = [f\"{i}round\" for i in range(1, 2)]"
      ],
      "metadata": {
        "id": "rgR3AKvvYhTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code"
      ],
      "metadata": {
        "id": "jOnpGyfEYkSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "def visualize_training_result(model):\n",
        "    y_pred = model.predict(x_train)\n",
        "    plot_confusion_matrix(confusion_matrix(y_train, y_pred), labels, title='Training result')\n",
        "\n",
        "def visualize_test_result(model):\n",
        "    y_pred = model.predict(x_test)\n",
        "    plot_confusion_matrix(confusion_matrix(y_test, y_pred), labels, title='Test result')\n",
        "\n",
        "def confusion_matrix_summary(model):\n",
        "    y_pred = model.predict(x_test)\n",
        "    (tn, fp, fn, tp) = confusion_matrix(y_test, y_pred).ravel()\n",
        "    n_total = tn + fp + fn + tp\n",
        "    return {\n",
        "        'accuracy': (tn + tp) / (tn + fp + fn + tp),\n",
        "        'sensitivity': tp / (tp + fn),\n",
        "        'specificity': tn / (tn + fp)\n",
        "    }\n",
        "\n",
        "def auc_summary(model, x_test, y_test):\n",
        "    y_pred = model.predict_proba(x_test)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_pred)\n",
        "    return auc\n",
        "\n",
        "def acc_summary(model, x_test, y_test):\n",
        "    y_pred = model.predict(x_test)\n",
        "    return np.mean(y_test == y_pred)"
      ],
      "metadata": {
        "id": "WZ4iOuCfYm1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "XUVCwV5QYqb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process inputs\n",
        "# raw_data = dict()\n",
        "# for round in rounds:\n",
        "#     raw_data[round] = pd.read_excel(p300_path / bandpass_filename, sheet_name=round, header=None, index_col=None)\n",
        "x_data = dict()\n",
        "y_data = dict()\n",
        "for i in tqdm(range(len(rounds)), desc=\"Loading excel data\"):\n",
        "    round = rounds[i]\n",
        "    raw_data = pd.read_excel(p300_path / bandpass_filename, sheet_name=round, header=None, index_col=None)\n",
        "    x_data[round] = raw_data.iloc[:, 1:]\n",
        "    assert x_data[round].shape[1] == max_input_dim\n",
        "    y_data[round] = raw_data.iloc[:, 0]"
      ],
      "metadata": {
        "id": "3DBa2sJKYqZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter"
      ],
      "metadata": {
        "id": "WcilKiNoYqWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auc_summary2(model, x_test, y_test, fn):\n",
        "    y_score = fn(model, x_test)\n",
        "    auc = roc_auc_score(y_test, y_score)\n",
        "    return auc"
      ],
      "metadata": {
        "id": "B85a1d6mYqT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from scipy import interp\n",
        "\n",
        "# def plot_roc_curve(model_builder, X, y, title=None, save=False, dir=p300_path):\n",
        "#     tprs = []\n",
        "#     aucs = []\n",
        "#     mean_fpr = np.linspace(0, 1, 101)\n",
        "#     for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "#         x_train = X.iloc[train_index, :]\n",
        "#         y_train = y.iloc[train_index]\n",
        "#         x_test = X.iloc[test_index, :]\n",
        "#         y_test = y.iloc[test_index]\n",
        "#         if isinstance(model_builder, tuple):\n",
        "#             model = model_builder[0]()\n",
        "#             model.fit(x_train, y_train)\n",
        "#             y_score = model_builder[1](model, x_test)\n",
        "#         else:\n",
        "#             model = model_builder()\n",
        "#             model.fit(x_train, y_train)\n",
        "#             y_score = model.predict_proba(x_test)[:, 1]\n",
        "#         fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "#         tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "#         roc_auc = auc(fpr, tpr)\n",
        "#         aucs.append(roc_auc)\n",
        "#         plt.plot(fpr, tpr, lw=2, alpha=0.3, label=f'ROC fold {i+1} (AUC = {roc_auc:.2f})')\n",
        "#     mean_tpr = np.mean(tprs, axis=0)\n",
        "#     mean_auc = auc(mean_fpr, mean_tpr)\n",
        "#     plt.plot(mean_fpr, mean_tpr, color='blue', label=f'Mean ROC (AUC = {mean_auc:.2f})', lw=2, alpha=1)\n",
        "#     plt.xlabel('False Positive Rate')\n",
        "#     plt.ylabel('True Positive Rate')\n",
        "#     plt.title(title)\n",
        "#     plt.legend(loc='lower right')\n",
        "#     plt.show()\n",
        "#     if save:\n",
        "#         df = pd.DataFrame({'mean_fpr': mean_fpr, 'mean_tpr': mean_tpr})\n",
        "#         with pd.ExcelWriter(dir / f'{title}.xlsx', engine='xlsxwriter') as writer:\n",
        "#             df.to_excel(writer)\n",
        "#             writer.save()\n",
        "#     return mean_tpr, mean_auc\n",
        "\n",
        "def plot_roc_curve(model_builder, X, y, input_dim, title=None, save=False, dir=p300_path):\n",
        "    tprs = []\n",
        "    aucs = []\n",
        "    mean_fpr = np.linspace(0, 1, 101)\n",
        "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "        x_train = X.iloc[train_index, :]\n",
        "        y_train = y.iloc[train_index]\n",
        "        x_test = X.iloc[test_index, :]\n",
        "        y_test = y.iloc[test_index]\n",
        "        if isinstance(model_builder, tuple):\n",
        "            #model = model_builder[0]()\n",
        "            model = model_builder[0](input_dim)\n",
        "            model.fit(x_train, y_train)\n",
        "            y_score = model_builder[1](model, x_test)\n",
        "        else:\n",
        "            #model = model_builder()\n",
        "            model = model_builder(input_dim)\n",
        "            model.fit(x_train, y_train)\n",
        "            y_score = model.predict_proba(x_test)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        aucs.append(roc_auc)\n",
        "        plt.plot(fpr, tpr, lw=2, alpha=0.3, label=f'ROC fold {i+1} (AUC = {roc_auc:.2f})')\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    plt.plot(mean_fpr, mean_tpr, color='blue', label=f'Mean ROC (AUC = {mean_auc:.2f})', lw=2, alpha=1)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(title)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "    if save:\n",
        "        if title is None:\n",
        "            raise RuntimeError(\"Title should be given.\")\n",
        "        df = pd.DataFrame({'mean_fpr': mean_fpr, 'mean_tpr': mean_tpr})\n",
        "        # writer = pd.ExcelWriter(dir / f'{title}.xlsx', engine='xlsxwriter')\n",
        "        # df.to_excel(writer)\n",
        "        # writer.save()\n",
        "        with pd.ExcelWriter(dir / f'{title}.xlsx', engine='xlsxwriter') as writer:\n",
        "            df.to_excel(writer)\n",
        "            writer.save()\n",
        "    return mean_tpr, mean_auc"
      ],
      "metadata": {
        "id": "nGtQ57cTYp2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "sngx2pAVYpw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "input_dims = [25]\n",
        "# builders = {\n",
        "#     'LR': lambda: LogisticRegression(),\n",
        "#     'SVM': (lambda: LinearSVC(), lambda m, x: m.decision_function(x)),\n",
        "#     'RF': lambda: RandomForestClassifier(),\n",
        "#     'kNN': lambda: KNeighborsClassifier(n_neighbors=5),\n",
        "#     'LDA': lambda: LinearDiscriminantAnalysis(),\n",
        "#     'QDA': lambda: QuadraticDiscriminantAnalysis()\n",
        "# }\n",
        "builders = {\n",
        "    'LR': lambda dim: LogisticRegression(),\n",
        "    'SVM': (lambda dim: SVC(kernel='linear'), lambda m, x: m.decision_function(x)),\n",
        "    'RF': lambda dim: RandomForestClassifier(n_estimators=128, max_features=input_dims[0], bootstrap=False, random_state=100),\n",
        "    'kNN': lambda dim: KNeighborsClassifier(n_neighbors=5),\n",
        "    'LDA': lambda dim: LinearDiscriminantAnalysis(),\n",
        "    'QDA': lambda dim: QuadraticDiscriminantAnalysis()\n",
        "}\n",
        "metrics = ['AUC']\n",
        "subject = 'SH'\n",
        "rounds = ['1round']\n",
        "lr_best_idx = {}\n",
        "for metric in metrics:\n",
        "    lr_best_idx[metric] = {}\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "for round in rounds:\n",
        "    for metric in metrics:\n",
        "        for input_dim in input_dims:\n",
        "            tprs = {}\n",
        "            mean_aucs = {}\n",
        "            for model_name, model_builder in builders.items():\n",
        "                save_dir = p300_path / f'{subject}_{metric}_INPUT_DIM_{input_dim}'\n",
        "                # if save_dir.exists():\n",
        "                #     shutil.rmtree(save_dir)\n",
        "                save_dir.mkdir(exist_ok=True)\n",
        "                accs = []\n",
        "                aucs = []\n",
        "                x_starts = [i for i in range(0, max_input_dim + 1 - input_dim)]\n",
        "                for j in tqdm(range(len(x_starts)), desc=f\"{subject}_{input_dim}_{model_name}_{round}_{metric}\"):\n",
        "                    accs.append([])\n",
        "                    aucs.append([])\n",
        "                    X = x_data[round].iloc[:, x_starts[j]:x_starts[j] + input_dim]\n",
        "                    y = y_data[round]\n",
        "                    for train_index, test_index in skf.split(X, y):\n",
        "                        x_train = X.iloc[train_index, :]\n",
        "                        y_train = y.iloc[train_index]\n",
        "                        x_test = X.iloc[test_index, :]\n",
        "                        y_test = y.iloc[test_index]\n",
        "                        if isinstance(model_builder, tuple):\n",
        "                            #model = model_builder[0]()\n",
        "                            model = model_builder[0](input_dim)\n",
        "                            model.fit(x_train, y_train)\n",
        "                            aucs[j].append(auc_summary2(model, x_test, y_test, model_builder[1]))\n",
        "                        else:\n",
        "                            #model = model_builder()\n",
        "                            model = model_builder(input_dim)\n",
        "                            model.fit(x_train, y_train)\n",
        "                            aucs[j].append(auc_summary(model, x_test, y_test))\n",
        "                        accs[j].append(acc_summary(model, x_test, y_test))\n",
        "                    accs[j] = np.mean(accs[j])\n",
        "                    aucs[j] = np.mean(aucs[j])\n",
        "                if metric == 'ACC':\n",
        "                    best_idx = np.argmax(accs)\n",
        "                    metric_score = accs[best_idx]\n",
        "                elif metric == 'AUC':\n",
        "                    best_idx = np.argmax(aucs)\n",
        "                    metric_score = aucs[best_idx]\n",
        "                else:\n",
        "                    raise RuntimeError(f\"Unknown metric {metric}\")\n",
        "                if model_name == 'LR':\n",
        "                    lr_best_idx[metric][input_dim] = best_idx\n",
        "                print(f'[Input dim {input_dim}][{model_name}] best {metric}: {metric_score} at index: {best_idx}')\n",
        "                X = x_data[round].iloc[:, x_starts[best_idx]:x_starts[best_idx] + input_dim]\n",
        "                y = y_data[round]\n",
        "                mean_tpr, mean_auc = plot_roc_curve(model_builder, X, y, input_dim, title=f'{subject}_{round}_{model_name}_BEST_{metric}_ROC_CURVE_INPUT_DIM_{input_dim}_INDEX_{best_idx}', save=True, dir=save_dir)\n",
        "                tprs[model_name] = mean_tpr\n",
        "                mean_aucs[model_name] = mean_auc\n",
        "            mean_fpr = np.linspace(0, 1, 101)\n",
        "            for model_name, mean_tpr in tprs.items():\n",
        "                mean_auc = mean_aucs[model_name]\n",
        "                plt.plot(mean_fpr, mean_tpr, label=f'{model_name} (AUC = {mean_auc:.2f})', lw=2, alpha=1)\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title(f\"Input dim: {input_dim}\")\n",
        "            plt.legend(loc='lower right')\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "sQ7BpckbYyxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for round in rounds:\n",
        "    for metric in metrics:\n",
        "        for input_dim in input_dims:\n",
        "            curr_index = lr_best_idx[metric][input_dim]\n",
        "            save_dir = p300_path / f'{subject}_{metric}_INPUT_DIM_{input_dim}_INDEX_{curr_index}'\n",
        "            # if save_dir.exists():\n",
        "            #     shutil.rmtree(save_dir)\n",
        "            save_dir.mkdir(exist_ok=True)\n",
        "            tprs = {}\n",
        "            mean_aucs = {}\n",
        "            for model_name, model_builder in builders.items():\n",
        "                accs = []\n",
        "                aucs = []\n",
        "                x_starts = [curr_index]\n",
        "                for j in tqdm(range(len(x_starts)), desc=f\"{subject}_{input_dim}_{model_name}_{round}_{metric}\"):\n",
        "                    accs.append([])\n",
        "                    aucs.append([])\n",
        "                    X = x_data[round].iloc[:, x_starts[j]:x_starts[j] + input_dim]\n",
        "                    y = y_data[round]\n",
        "                    for train_index, test_index in skf.split(X, y):\n",
        "                        x_train = X.iloc[train_index, :]\n",
        "                        y_train = y.iloc[train_index]\n",
        "                        x_test = X.iloc[test_index, :]\n",
        "                        y_test = y.iloc[test_index]\n",
        "                        if isinstance(model_builder, tuple):\n",
        "                            #model = model_builder[0]()\n",
        "                            model = model_builder[0](input_dim)\n",
        "                            model.fit(x_train, y_train)\n",
        "                            aucs[j].append(auc_summary2(model, x_test, y_test, model_builder[1]))\n",
        "                        else:\n",
        "                            #model = model_builder()\n",
        "                            model = model_builder(input_dim)\n",
        "                            model.fit(x_train, y_train)\n",
        "                            aucs[j].append(auc_summary(model, x_test, y_test))\n",
        "                        accs[j].append(acc_summary(model, x_test, y_test))\n",
        "                    accs[j] = np.mean(accs[j])\n",
        "                    aucs[j] = np.mean(aucs[j])\n",
        "                if metric == 'ACC':\n",
        "                    best_idx = np.argmax(accs)\n",
        "                    metric_score = accs[best_idx]\n",
        "                elif metric == 'AUC':\n",
        "                    best_idx = np.argmax(aucs)\n",
        "                    metric_score = aucs[best_idx]\n",
        "                else:\n",
        "                    raise RuntimeError(f\"Unknown metric {metric}\")\n",
        "                #print(f'[Input dim {input_dim}][{model_name}] best {metric}: {metric_score} at index: {best_idx}')\n",
        "                X = x_data[round].iloc[:, x_starts[best_idx]:x_starts[best_idx] + input_dim]\n",
        "                y = y_data[round]\n",
        "                mean_tpr, mean_auc = plot_roc_curve(model_builder, X, y, input_dim, title=f'{subject}_{round}_{model_name}_BEST_{metric}_ROC_CURVE_INPUT_DIM_{input_dim}_INDEX_{x_starts[best_idx]}', save=True, dir=save_dir)\n",
        "                tprs[model_name] = mean_tpr\n",
        "                mean_aucs[model_name] = mean_auc\n",
        "            mean_fpr = np.linspace(0, 1, 101)\n",
        "            for model_name, mean_tpr in tprs.items():\n",
        "                mean_auc = mean_aucs[model_name]\n",
        "                plt.plot(mean_fpr, mean_tpr, label=f'{model_name} (AUC = {mean_auc:.2f})', lw=2, alpha=1)\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title(f\"Input dim: {input_dim}\")\n",
        "            plt.legend(loc='lower right')\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "0VXeCo8dY1Ch"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}