{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2mrm7_da3rCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup environment\n",
        "!apt-get -qq install xxd\n",
        "!pip install pandas numpy matplotlib\n",
        "!pip install tensorflow==2.0.0-rc1\n",
        "!pip install tensorflow-gpu==2.0.0\n",
        "!pip install sklearn"
      ],
      "metadata": {
        "id": "VYNxnLv5p8Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "filename = \"tar_10.csv\"  #target file\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/revise/\" + filename)\n",
        "length = df.shape[1]\n",
        "print(length)\n",
        "df2 = df.iloc[5:35,0]  #set input dimension\n",
        "for x in range(length-1) :\n",
        "  df2=pd.concat([df2,df.iloc[5:35,x+1]])\n",
        "\n",
        "print(df2)\n",
        "\n",
        "df2.to_csv(\"tar.csv\")\n",
        "\n",
        "filename = \"non_10.csv\"  #target file\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/revise/\" + filename)\n",
        "length= df.shape[1]\n",
        "print(length)\n",
        "df2 = df.iloc[5:35,0]   #set input dimension\n",
        "for x in range(length-1) :\n",
        "  df2=pd.concat([df2,df.iloc[5:35,x+1]])\n",
        "\n",
        "print(df2)\n",
        "\n",
        "df2.to_csv(\"non.csv\")\n"
      ],
      "metadata": {
        "id": "yIj3kBWueTGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvsNth0YggVi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "print(f\"TensorFlow version = {tf.__version__}\\n\")\n",
        "\n",
        "# Set a fixed random seed value, for reproducibility, this will allow us to get\n",
        "# the same random numbers each time the notebook is run\n",
        "SEED = 1337\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# the list of pattern that data is available for\n",
        "PATTERNS = [\n",
        "    \"tar\",\n",
        "    \"non\",\n",
        "]\n",
        "\n",
        "SAMPLES_PER_PATTERN = 30\n",
        "\n",
        "NUM_PATTERNS = 2\n",
        "\n",
        "# create a one-hot encoded matrix that is used in the output\n",
        "ONE_HOT_ENCODED_PATTERNS = np.eye(NUM_PATTERNS)\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "# read each csv file and push an input and output\n",
        "for pattern_index in range(NUM_PATTERNS):\n",
        "  pattern = PATTERNS[pattern_index]\n",
        "  print(f\"Processing index {pattern_index} for pattern '{pattern}'.\")\n",
        "  \n",
        "  output = ONE_HOT_ENCODED_PATTERNS[pattern_index]\n",
        "  \n",
        "  df = pd.read_csv(pattern + \".csv\")\n",
        "  \n",
        "  # calculate the number of pattern recordings in the file\n",
        "  num_recordings = int(df.shape[0] / SAMPLES_PER_PATTERN)\n",
        "  \n",
        "  print(f\"\\tThere are {num_recordings} recordings of the {pattern} pattern.\")\n",
        "  \n",
        "  for i in range(num_recordings):\n",
        "    tensor = []\n",
        "    for j in range(SAMPLES_PER_PATTERN):\n",
        "      index = i * SAMPLES_PER_PATTERN + j\n",
        "      # normalize the input data, between 0 to 1:\n",
        "      # - acceleration is between: -4 to +4\n",
        "      # - gyroscope is between: -2000 to +2000\n",
        "      tensor += [\n",
        "         (df['0'][index])/200\n",
        "      ]\n",
        "\n",
        "    inputs.append(tensor)\n",
        "    outputs.append(output)\n",
        "\n",
        "# convert the list to numpy array\n",
        "inputs = np.array(inputs)\n",
        "outputs = np.array(outputs)\n",
        "\n",
        "print(\"Data set parsing and preparation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7t1IWPIggVj",
        "outputId": "6cb021f3-e126-447c-b6ab-8108b92d2139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data set randomization and splitting complete.\n"
          ]
        }
      ],
      "source": [
        "num_inputs = len(inputs)\n",
        "randomize = np.arange(num_inputs)\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        " Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\n",
        "inputs = inputs[randomize]\n",
        "outputs = outputs[randomize]\n",
        "\n",
        "# Split the recordings (group of samples) into three sets: training, testing and validation\n",
        "TRAIN_SPLIT = int(0.6 * num_inputs)\n",
        "TEST_SPLIT = int(0.2 * num_inputs )\n",
        "print(TRAIN_SPLIT)\n",
        "print(TEST_SPLIT) \n",
        "\n",
        "inputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "outputs_train, outputs_test, outputs_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "\n",
        "print(\"Data set randomization and splitting complete.\")\n",
        "\n",
        "\"\"\"\n",
        "# 5-Cross validaion\n",
        "#1\n",
        "TEST_1 = 0\n",
        "TEST_2 = 30\n",
        "\n",
        "inputs_train_a, inputs_test_a, inputs_train2_a = np.split(inputs, [TEST_1,TEST_2])\n",
        "outputs_train_a, outputs_test_a, outputs_train2_a = np.split(outputs, [TEST_1,TEST_2])\n",
        "\n",
        "inputs_train_a = np.vstack((inputs_train_a, inputs_train2_a))\n",
        "outputs_train_a = np.vstack((outputs_train_a, outputs_train2_a))\n",
        "\n",
        "#2\n",
        "TEST_1 = 30\n",
        "TEST_2 = 60\n",
        "\n",
        "inputs_train_b, inputs_test_b, inputs_train2_b = np.split(inputs, [TEST_1,TEST_2])\n",
        "outputs_train_b, outputs_test_b, outputs_train2_b = np.split(outputs, [TEST_1,TEST_2])\n",
        "\n",
        "inputs_train_b = np.vstack((inputs_train_b, inputs_train2_b))\n",
        "outputs_train_b = np.vstack((outputs_train_b, outputs_train2_b))\n",
        "\n",
        "#3\n",
        "TEST_1 = 60\n",
        "TEST_2 = 90\n",
        "\n",
        "inputs_train_c, inputs_test_c, inputs_train2_c = np.split(inputs, [TEST_1,TEST_2])\n",
        "outputs_train_c, outputs_test_c, outputs_train2_c = np.split(outputs, [TEST_1,TEST_2])\n",
        "\n",
        "inputs_train_c = np.vstack((inputs_train_c, inputs_train2_c))\n",
        "outputs_train_c = np.vstack((outputs_train_c, outputs_train2_c))\n",
        "\n",
        "#4\n",
        "TEST_1 = 90\n",
        "TEST_2 = 119\n",
        "\n",
        "inputs_train_d, inputs_test_d, inputs_train2_d = np.split(inputs, [TEST_1,TEST_2])\n",
        "outputs_train_d, outputs_test_d, outputs_train2_d = np.split(outputs, [TEST_1,TEST_2])\n",
        "\n",
        "inputs_train_d = np.vstack((inputs_train_d, inputs_train2_d))\n",
        "outputs_train_d = np.vstack((outputs_train_d, outputs_train2_d))\n",
        "\n",
        "#5\n",
        "TEST_1 = 119\n",
        "TEST_2 = 148\n",
        "\n",
        "inputs_train_e, inputs_test_e, inputs_train2_e = np.split(inputs, [TEST_1,TEST_2])\n",
        "outputs_train_e, outputs_test_e, outputs_train2_e = np.split(outputs, [TEST_1,TEST_2])\n",
        "\n",
        "inputs_train_e = np.vstack((inputs_train_e, inputs_train2_e))\n",
        "outputs_train_e = np.vstack((outputs_train_e, outputs_train2_e))\n",
        "\n",
        "print(\"Data set randomization and splitting complete.\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGbOdufgggVk"
      },
      "outputs": [],
      "source": [
        "num_Train = 0.6 * num_inputs\n",
        "num_Test = 0.2 * num_inputs\n",
        "num_Validate = 0.2 * num_inputs\n",
        "\n",
        "inputs_train_LSTM = np.reshape(inputs_train, (num_Train, SAMPLES_PER_PATTERNS, 1))\n",
        "inputs_validate_LSTM = np.reshape(inputs_validate, (num_Validate, SAMPLES_PER_PATTERNS, 1))\n",
        "inputs_test_LSTM = np.reshape(inputs_test, (num_Test, SAMPLES_PER_PATTERNS, 1))\n",
        "\n",
        "\"\"\"\n",
        "inputs_train_LSTM_a = np.reshape(inputs_train_a, (num_Train, SAMPLES_PER_PATTERN, 1))\n",
        "inputs_test_LSTM_a = np.reshape(inputs_test_a, (num_Test, SAMPLES_PER_PATTERN, 1))\n",
        "\n",
        "inputs_train_LSTM_b = np.reshape(inputs_train_b, (num_Train, SAMPLES_PER_PATTERN, 1))\n",
        "inputs_test_LSTM_b = np.reshape(inputs_test_b, (num_Test, SAMPLES_PER_PATTERN, 1))\n",
        "\n",
        "inputs_train_LSTM_c = np.reshape(inputs_train_c, (num_Train, SAMPLES_PER_PATTERN, 1))\n",
        "inputs_test_LSTM_c = np.reshape(inputs_test_c, (num_Test, SAMPLES_PER_PATTERN, 1))\n",
        "\n",
        "inputs_train_LSTM_d = np.reshape(inputs_train_d, (num_Train, SAMPLES_PER_PATTERN, 1))\n",
        "inputs_test_LSTM_d = np.reshape(inputs_test_d, (num_Test, SAMPLES_PER_PATTERN, 1))\n",
        "\n",
        "inputs_train_LSTM_e = np.reshape(inputs_train_e, (num_Train, SAMPLES_PER_PATTERN, 1))\n",
        "inputs_test_LSTM_e = np.reshape(inputs_test_e, (num_Test, SAMPLES_PER_PATTERN, 1))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.shape)\n",
        "print(inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ1jaTTK0Bwm",
        "outputId": "1e0ac1a3-fd98-49e4-b492-c2e273403264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(148, 2)\n",
            "(148, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DNN for ErrP\n",
        "\n",
        "from tensorflow.keras.metrics import AUC\n",
        "\n",
        "for i in range(10):\n",
        "  #with tf.device('/device:GPU:0'):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(2, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=[AUC(name='auc'),'accuracy'])\n",
        "    csv_logger = tf.keras.callbacks.CSVLogger('/content/drive/MyDrive/revise/training.csv', append = True)\n",
        "    history = model.fit(inputs_train, outputs_train, epochs=33, batch_size=10, validation_data=(inputs_validate, outputs_validate), callbacks = [csv_logger])"
      ],
      "metadata": {
        "id": "SCzsSi1WWIrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM for ErrP\n",
        "\n",
        "from tensorflow.keras.metrics import AUC\n",
        "\n",
        "for i in range(10):\n",
        "  #with tf.device('/device:GPU:0'):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.LSTM(90, input_shape=(SAMPLES_PER_PATTERNS,1)))\n",
        "    model.add(tf.keras.layers.Dense(18, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(2, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=[AUC(name='auc'),'accuracy'])\n",
        "    csv_logger = tf.keras.callbacks.CSVLogger('/content/drive/MyDrive/revise/training_LSTM.csv', append = True)\n",
        "    history = model.fit(inputs_train_LSTM, outputs_train, epochs=150, batch_size=10, validation_data=(inputs_test_LSTM_b, outputs_test_b), callbacks = [csv_logger])"
      ],
      "metadata": {
        "id": "w1AxQMCrWk-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw4YApmyggVs"
      },
      "outputs": [],
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"PATTERNS_model.tflite\", \"wb\").write(tflite_model)\n",
        "  \n",
        "import os\n",
        "basic_model_size = os.path.getsize(\"PATTERNS_model.tflite\")\n",
        "print(\"Model is %d bytes\" % basic_model_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Suw_x43ZggVs"
      },
      "outputs": [],
      "source": [
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat gesture_model.tflite | xxd -i      >> /content/model.h\n",
        "!echo \"};\"                              >> /content/model.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "ErrP_deeplearning의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}